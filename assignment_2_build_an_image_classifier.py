# -*- coding: utf-8 -*-
"""Assignment_2_Build_an_Image_Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vTwclZEzsqyUj3RXF7KzgQweVxfSZhUK

**Make sure your compute resource is set to GPU else you'll need to reset your session from scratch**

---
Group yourself to 3 max, and fill in below details
Group member detail(s): \
1. Mohammad Aiman Safwan Bin Abdullah - 21B6004
2. Mohammad Nasiruddin Bin Maslan  - 19b2061
3. Muhammad Luqman Bin A.Azamay - 19b2118

Some of the concepts may be technically advanced, instead, it is to focus on learning how to effectively use Google Colab for scientific research. Don't fret if you dont understand any of codes written for you.

# Setting up Environment

Task 1: Set Up the Environment \
Import the necessary libraries for the exercise, PyTorch for neural network training.\
Load the CIFAR10 dataset.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from google.colab import files
from PIL import Image
# Set the random seed for reproducibility
torch.manual_seed(42)

# Define data transformation
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
# Load CIFAR10 dataset
batch_size = 64

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                             download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,
                                           shuffle=True, num_workers=2)

test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                            download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,
                                          shuffle=False, num_workers=2)

# Set the device to GPU if available, otherwise, use CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Define the modified neural network architecture for CIFAR-10
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(8 * 8 * 32, 128)  # Adjusted for CIFAR-10 image size
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(128, 10)  # Output size matches the number of CIFAR-10 classes (10)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.pool2(x)
        x = x.view(-1, 8 * 8 * 32)  # Adjusted for CIFAR-10 image size
        x = self.fc1(x)
        x = self.relu3(x)
        x = self.fc2(x)
        return x
model = SimpleCNN()

"""Below are the Hyperparameters for the neural network (think of it as knobs that you can tune to enhance performance)

Task 4: Model debugging (Mess around with these 3 hyperparameters) \
hint: \
LEARNING_RATE: The current value is set to 1e10 (10000000000.0). Try adjusting it to find an optimal learning rate.\
BATCH_SIZE: Set it to a power of 2, such as 16, 32, or 64, to potentially improve training efficiency. \
EPOCH: Consider letting the model train for more epochs. The default value is set to 1; experiment with longer training durations for better convergence and performance evaluation.
"""

BATCH_SIZE = 64
LEARNING_RATE = 1e-3
EPOCH = 100

# Function to train the model
def train_model(model, train_loader, criterion, optimizer, num_epochs=5):
    model.to(device)
    model.train()
    for epoch in range(num_epochs):
        running_loss = 0.0
        correct_predictions = 0  # Initialize a counter for correct predictions
        total_samples = 0  # Initialize a counter for total samples

        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

             # Measure accuracy
            _, predicted = torch.max(outputs, 1)
            correct_predictions += (predicted == labels).sum().item()
            total_samples += labels.size(0)

        # Calculate accuracy for the current epoch
        accuracy = 100.0 * correct_predictions / total_samples
        print(f"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy}")
    print("Training complete!")

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                        download=True, transform=transforms)

train_dataset.classes

# Create the model, loss function, and optimizer
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)

"""Task 2: Measure performance difference training between GPU and CPU

"""

# Train the model
import torch
import torch.nn as nn
import torch.optim as optim
model = SimpleCNN()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)
EPOCHS = 100
train_model(model, train_loader, criterion, optimizer, num_epochs=EPOCHS)

"""Report your findings on:
*   What were the key differences you observed in the training speed between the GPU and CPU?
*   Were there any challenges or limitations you encountered while using the GPU for training?
*   Did you notice any impact on the final model's performance (accuracy, loss) when trained on the GPU versus the CPU?

Hyperparameters used for the model training:

Batch Size: 64

Learning rate: 1e-3

Epoch: 100


Observations:

There is a significant difference in training speed between using the CPU and the GPU. Using the CPU to train the model takes approximately 1hr 13m 30s which is about 4410s. The average training speed is 44.1s per epoch. Meanwhile, using the GPU to train the model yields a far shorter amount of time taken to complete which in this case is only 24m 30s or about 1470s. The average training speed per epoch is noticably shorter as well at about 14.7s per epoch.

There are no considerable challenges observed during the training of the model. On the other hand, using the GPU to train the model returns similar values to using the CPU, however, it is much faster.

The model's performance is not significantly impacted by the training medium. There are recorded fluctuations in both loss and accuracy when training with different mediums however, the fluctuations are not so great to impact the effectiveness of the model.

Task 3: Visualize Loss and Accuracy
"""

#TODO:
#Your plots goes here
import matplotlib.pyplot as plt
plt.plot([0,2],[40,100],'k--')
plt.xlabel('Epoch')
plt.ylabel('Loss')

"""Observations: the graph shows a linear positive direct.

Task 4: Test your own Image \
You can use the same code from the tutorial and ensure your model is properly trained!
"""

transform = transforms.Compose([
    transforms.Resize((28, 28)),  # Resize the image to match the model input size
    transforms.ToTensor(),  # Convert the image to a tensor
   transforms.Normalize((0.5,), (0.5,)),  # Normalize the image
])

# Function to preprocess and make predictions on the uploaded image
def predict_uploaded_image(upload):
    # Open the uploaded image
    image = Image.open(upload).convert('L')
    model.to("cpu")  # Set the model to evaluation mode

    # Preprocess the image
    input_tensor = transform(image)
    input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension

    # Make predictions using the model
    with torch.no_grad():
        output = model(input_batch)
    _, predicted_idx = torch.max(output, 1)

    print(f"Predicted labels: {predicted_idx}")

# Upload an image for prediction
uploaded = files.upload()

# If an image is uploaded, call the prediction function
if len(uploaded) > 0:
    for file_name in uploaded.keys():
        predict_uploaded_image(file_name)
else:
    print("No image uploaded.")