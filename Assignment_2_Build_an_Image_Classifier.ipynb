{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChihuahuaHunter/SDS3201-AssignmentColab/blob/master/Assignment_2_Build_an_Image_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M47oZUfDT4u"
      },
      "source": [
        "**Make sure your compute resource is set to GPU else you'll need to reset your session from scratch**\n",
        "\n",
        "---\n",
        "Group yourself to 3 max, and fill in below details\n",
        "Group member detail(s): \\\n",
        "1. Mohammad Aiman Safwan Bin Abdullah - 21B6004\n",
        "2. Mohammad Nasiruddin Bin Maslan  - 19b2061\n",
        "3. Muhammad Luqman Bin A.Azamay - 19b2118\n",
        "\n",
        "Some of the concepts may be technically advanced, instead, it is to focus on learning how to effectively use Google Colab for scientific research. Don't fret if you dont understand any of codes written for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xprg08vEQqD"
      },
      "source": [
        "# Setting up Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAOS-EJzEavg"
      },
      "source": [
        "Task 1: Set Up the Environment \\\n",
        "Import the necessary libraries for the exercise, PyTorch for neural network training.\\\n",
        "Load the CIFAR10 dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qZrtDipATDh",
        "outputId": "08a2b613-3e94-42a1-eccd-e6248e7fc50a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:14<00:00, 12109843.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Define data transformation\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "# Load CIFAR10 dataset\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                             download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
        "                                           shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                            download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J23xkfU4AVpC"
      },
      "outputs": [],
      "source": [
        "# Set the device to GPU if available, otherwise, use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1bdybnT-Z1j"
      },
      "outputs": [],
      "source": [
        "# Define the modified neural network architecture for CIFAR-10\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 32, 128)  # Adjusted for CIFAR-10 image size\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output size matches the number of CIFAR-10 classes (10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.view(-1, 8 * 8 * 32)  # Adjusted for CIFAR-10 image size\n",
        "        # x = x.view(x.size(0), -1)   # Use for own image\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "model = SimpleCNN()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KACp73JDFg57"
      },
      "source": [
        "Below are the Hyperparameters for the neural network (think of it as knobs that you can tune to enhance performance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmsOBQ1kXFHA"
      },
      "source": [
        "Task 4: Model debugging (Mess around with these 3 hyperparameters) \\\n",
        "hint: \\\n",
        "LEARNING_RATE: The current value is set to 1e10 (10000000000.0). Try adjusting it to find an optimal learning rate.\\\n",
        "BATCH_SIZE: Set it to a power of 2, such as 16, 32, or 64, to potentially improve training efficiency. \\\n",
        "EPOCH: Consider letting the model train for more epochs. The default value is set to 1; experiment with longer training durations for better convergence and performance evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDlnlmgKFhiK"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 1e-3\n",
        "EPOCH = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5q7aHCZAZWw"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        correct_predictions = 0  # Initialize a counter for correct predictions\n",
        "        total_samples = 0  # Initialize a counter for total samples\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "             # Measure accuracy\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "        # Calculate accuracy for the current epoch\n",
        "        accuracy = 100.0 * correct_predictions / total_samples\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Accuracy: {accuracy}\")\n",
        "    print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k59nYWhsB1n0",
        "outputId": "93dec724-5608-475e-8ed3-03951dcd00a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9uu5lfFF-FH",
        "outputId": "e43c83e2-a9e9-4672-9225-e93ec9267244"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "train_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUYLg9WRAdUp"
      },
      "outputs": [],
      "source": [
        "# Create the model, loss function, and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8Yh4J-wUR1W"
      },
      "source": [
        "Task 2: Measure performance difference training between GPU and CPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teF6NB2dAeV-",
        "outputId": "791508bd-1ef4-4114-9350-de961e76263c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 2.1429, Accuracy: 22.786\n",
            "Epoch 2/100, Loss: 1.7935, Accuracy: 36.67\n",
            "Epoch 3/100, Loss: 1.5738, Accuracy: 44.232\n",
            "Epoch 4/100, Loss: 1.4246, Accuracy: 49.256\n",
            "Epoch 5/100, Loss: 1.3336, Accuracy: 52.55\n",
            "Epoch 6/100, Loss: 1.2614, Accuracy: 55.08\n",
            "Epoch 7/100, Loss: 1.2029, Accuracy: 57.266\n",
            "Epoch 8/100, Loss: 1.1564, Accuracy: 59.03\n",
            "Epoch 9/100, Loss: 1.1097, Accuracy: 60.794\n",
            "Epoch 10/100, Loss: 1.0644, Accuracy: 62.658\n",
            "Epoch 11/100, Loss: 1.0255, Accuracy: 63.914\n",
            "Epoch 12/100, Loss: 0.9851, Accuracy: 65.554\n",
            "Epoch 13/100, Loss: 0.9480, Accuracy: 66.6\n",
            "Epoch 14/100, Loss: 0.9153, Accuracy: 67.962\n",
            "Epoch 15/100, Loss: 0.8817, Accuracy: 69.08\n",
            "Epoch 16/100, Loss: 0.8528, Accuracy: 70.16\n",
            "Epoch 17/100, Loss: 0.8214, Accuracy: 71.35\n",
            "Epoch 18/100, Loss: 0.7918, Accuracy: 72.33\n",
            "Epoch 19/100, Loss: 0.7652, Accuracy: 73.22\n",
            "Epoch 20/100, Loss: 0.7404, Accuracy: 74.054\n",
            "Epoch 21/100, Loss: 0.7126, Accuracy: 75.344\n",
            "Epoch 22/100, Loss: 0.6851, Accuracy: 76.122\n",
            "Epoch 23/100, Loss: 0.6619, Accuracy: 76.988\n",
            "Epoch 24/100, Loss: 0.6352, Accuracy: 78.076\n",
            "Epoch 25/100, Loss: 0.6133, Accuracy: 78.706\n",
            "Epoch 26/100, Loss: 0.5929, Accuracy: 79.494\n",
            "Epoch 27/100, Loss: 0.5675, Accuracy: 80.33\n",
            "Epoch 28/100, Loss: 0.5395, Accuracy: 81.432\n",
            "Epoch 29/100, Loss: 0.5156, Accuracy: 82.27\n",
            "Epoch 30/100, Loss: 0.4943, Accuracy: 83.05\n",
            "Epoch 31/100, Loss: 0.4736, Accuracy: 83.562\n",
            "Epoch 32/100, Loss: 0.4441, Accuracy: 84.79\n",
            "Epoch 33/100, Loss: 0.4246, Accuracy: 85.444\n",
            "Epoch 34/100, Loss: 0.4053, Accuracy: 86.38\n",
            "Epoch 35/100, Loss: 0.3793, Accuracy: 87.016\n",
            "Epoch 36/100, Loss: 0.3595, Accuracy: 87.628\n",
            "Epoch 37/100, Loss: 0.3387, Accuracy: 88.566\n",
            "Epoch 38/100, Loss: 0.3186, Accuracy: 89.346\n",
            "Epoch 39/100, Loss: 0.2892, Accuracy: 90.438\n",
            "Epoch 40/100, Loss: 0.2740, Accuracy: 90.942\n",
            "Epoch 41/100, Loss: 0.2537, Accuracy: 91.652\n",
            "Epoch 42/100, Loss: 0.2353, Accuracy: 92.386\n",
            "Epoch 43/100, Loss: 0.2171, Accuracy: 92.968\n",
            "Epoch 44/100, Loss: 0.1934, Accuracy: 93.966\n",
            "Epoch 45/100, Loss: 0.1851, Accuracy: 94.164\n",
            "Epoch 46/100, Loss: 0.1640, Accuracy: 95.016\n",
            "Epoch 47/100, Loss: 0.1499, Accuracy: 95.576\n",
            "Epoch 48/100, Loss: 0.1368, Accuracy: 95.91\n",
            "Epoch 49/100, Loss: 0.1234, Accuracy: 96.372\n",
            "Epoch 50/100, Loss: 0.1089, Accuracy: 96.96\n",
            "Epoch 51/100, Loss: 0.0913, Accuracy: 97.67\n",
            "Epoch 52/100, Loss: 0.0779, Accuracy: 98.098\n",
            "Epoch 53/100, Loss: 0.0678, Accuracy: 98.524\n",
            "Epoch 54/100, Loss: 0.0617, Accuracy: 98.694\n",
            "Epoch 55/100, Loss: 0.0540, Accuracy: 98.948\n",
            "Epoch 56/100, Loss: 0.0437, Accuracy: 99.25\n",
            "Epoch 57/100, Loss: 0.0333, Accuracy: 99.618\n",
            "Epoch 58/100, Loss: 0.0259, Accuracy: 99.782\n",
            "Epoch 59/100, Loss: 0.0215, Accuracy: 99.866\n",
            "Epoch 60/100, Loss: 0.0185, Accuracy: 99.896\n",
            "Epoch 61/100, Loss: 0.0152, Accuracy: 99.946\n",
            "Epoch 62/100, Loss: 0.0128, Accuracy: 99.978\n",
            "Epoch 63/100, Loss: 0.0109, Accuracy: 99.986\n",
            "Epoch 64/100, Loss: 0.0097, Accuracy: 99.994\n",
            "Epoch 65/100, Loss: 0.0086, Accuracy: 99.998\n",
            "Epoch 66/100, Loss: 0.0079, Accuracy: 99.994\n",
            "Epoch 67/100, Loss: 0.0072, Accuracy: 99.998\n",
            "Epoch 68/100, Loss: 0.0069, Accuracy: 99.992\n",
            "Epoch 69/100, Loss: 0.0062, Accuracy: 99.996\n",
            "Epoch 70/100, Loss: 0.0057, Accuracy: 100.0\n",
            "Epoch 71/100, Loss: 0.0053, Accuracy: 100.0\n",
            "Epoch 72/100, Loss: 0.0050, Accuracy: 100.0\n",
            "Epoch 73/100, Loss: 0.0047, Accuracy: 100.0\n",
            "Epoch 74/100, Loss: 0.0045, Accuracy: 99.998\n",
            "Epoch 75/100, Loss: 0.0042, Accuracy: 100.0\n",
            "Epoch 76/100, Loss: 0.0041, Accuracy: 99.996\n",
            "Epoch 77/100, Loss: 0.0038, Accuracy: 99.996\n",
            "Epoch 78/100, Loss: 0.0036, Accuracy: 100.0\n",
            "Epoch 79/100, Loss: 0.0034, Accuracy: 100.0\n",
            "Epoch 80/100, Loss: 0.0033, Accuracy: 100.0\n",
            "Epoch 81/100, Loss: 0.0031, Accuracy: 100.0\n",
            "Epoch 82/100, Loss: 0.0030, Accuracy: 100.0\n",
            "Epoch 83/100, Loss: 0.0029, Accuracy: 100.0\n",
            "Epoch 84/100, Loss: 0.0028, Accuracy: 100.0\n",
            "Epoch 85/100, Loss: 0.0027, Accuracy: 100.0\n",
            "Epoch 86/100, Loss: 0.0026, Accuracy: 100.0\n",
            "Epoch 87/100, Loss: 0.0025, Accuracy: 100.0\n",
            "Epoch 88/100, Loss: 0.0024, Accuracy: 100.0\n",
            "Epoch 89/100, Loss: 0.0023, Accuracy: 100.0\n",
            "Epoch 90/100, Loss: 0.0023, Accuracy: 100.0\n",
            "Epoch 91/100, Loss: 0.0022, Accuracy: 100.0\n",
            "Epoch 92/100, Loss: 0.0021, Accuracy: 100.0\n",
            "Epoch 93/100, Loss: 0.0021, Accuracy: 100.0\n",
            "Epoch 94/100, Loss: 0.0020, Accuracy: 100.0\n",
            "Epoch 95/100, Loss: 0.0019, Accuracy: 100.0\n",
            "Epoch 96/100, Loss: 0.0019, Accuracy: 100.0\n",
            "Epoch 97/100, Loss: 0.0018, Accuracy: 100.0\n",
            "Epoch 98/100, Loss: 0.0018, Accuracy: 100.0\n",
            "Epoch 99/100, Loss: 0.0017, Accuracy: 100.0\n",
            "Epoch 100/100, Loss: 0.0017, Accuracy: 100.0\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
        "EPOCHS = 100\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=EPOCHS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyN4gk65UUAl"
      },
      "source": [
        "Report your findings on:\n",
        "*   What were the key differences you observed in the training speed between the GPU and CPU?\n",
        "*   Were there any challenges or limitations you encountered while using the GPU for training?\n",
        "*   Did you notice any impact on the final model's performance (accuracy, loss) when trained on the GPU versus the CPU?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dRn_1s3WuWH"
      },
      "source": [
        "Hyperparameters used for the model training:\n",
        "\n",
        "Batch Size: 64\n",
        "\n",
        "Learning rate: 1e-3\n",
        "\n",
        "Epoch: 100\n",
        "\n",
        "\n",
        "Observations:\n",
        "\n",
        "There is a significant difference in training speed between using the CPU and the GPU. Using the CPU to train the model takes approximately 1hr 13m 30s which is about 4410s. The average training speed is 44.1s per epoch. Meanwhile, using the GPU to train the model yields a far shorter amount of time taken to complete which in this case is only 24m 4s or about 1444s. The average training speed per epoch is noticably shorter as well at about 14.4s per epoch.\n",
        "\n",
        "There are no considerable challenges observed during the training of the model. On the other hand, using the GPU to train the model returns similar values to using the CPU, however, it is much faster.\n",
        "\n",
        "The model's performance is not significantly impacted by the training medium. There are recorded fluctuations in both loss and accuracy when training with different mediums however, the fluctuations are not so great to impact the effectiveness of the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsInMplEVyCK"
      },
      "source": [
        "Task 3: Visualize Loss and Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "jGiiegGeVYEn",
        "outputId": "5b671849-a5f7-468c-a249-ed79f191bc77"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Accuracy')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#TODO:\n",
        "#Your plots goes here\n",
        "import matplotlib.pyplot as plt\n",
        "figure, axis = plt.subplots(1,2)\n",
        "\n",
        "# Epoch\n",
        "x = range(0,100)\n",
        "\n",
        "# Loss\n",
        "# y1 =    #take value for loss\n",
        "axis[0].set_title('Loss')\n",
        "# axis[0].plot(x,y1) #plot graph for loss\n",
        "\n",
        "# Accuracy\n",
        "# y2 =  #take value for accuracy\n",
        "axis[1].set_title('Accuracy')\n",
        "# axis[1].plot(x,y2) #plot graph for accuracy\n",
        "\n",
        "# x = Epoch [1-100]\n",
        "# y1 = Loss [2.5 - 0]\n",
        "# y2 = Accuracy [20 - 100]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zxt6eE_dkeV"
      },
      "source": [
        "Observations:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUNYsuwRX-VT"
      },
      "source": [
        "Task 4: Test your own Image \\\n",
        "You can use the same code from the tutorial and ensure your model is properly trained!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LN02H0eTYC68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "82dccb9b-5008-40ac-89d6-46ddc62b70a5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a5f69d4b0f08>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m transform = transforms.Compose([\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Resize the image to match the model input size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Convert the image to a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Normalize the image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ])\n",
            "\u001b[0;31mNameError\u001b[0m: name 'transforms' is not defined"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Resize the image to match the model input size\n",
        "    transforms.ToTensor(),  # Convert the image to a tensor\n",
        "   transforms.Normalize((0.5,), (0.5,)),  # Normalize the image\n",
        "])\n",
        "\n",
        "# Function to preprocess and make predictions on the uploaded image\n",
        "def predict_uploaded_image(upload):\n",
        "    # Open the uploaded image\n",
        "    image = Image.open(upload).convert('RGB')\n",
        "    model.to(\"cpu\")  # Set the model to evaluation mode\n",
        "\n",
        "    # Preprocess the image\n",
        "    input_tensor = transform(image)\n",
        "    input_batch = input_tensor.unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "    # Make predictions using the model\n",
        "    with torch.no_grad():\n",
        "        output = model(input_batch)\n",
        "    _, predicted_idx = torch.max(output, 1)\n",
        "\n",
        "    print(f\"Predicted labels: {predicted_idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zejpODAGC4DA"
      },
      "outputs": [],
      "source": [
        "# Upload an image for prediction\n",
        "uploaded = files.upload()\n",
        "\n",
        "# If an image is uploaded, call the prediction function\n",
        "if len(uploaded) > 0:\n",
        "    for file_name in uploaded.keys():\n",
        "        predict_uploaded_image(file_name)\n",
        "else:\n",
        "    print(\"No image uploaded.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}